# -*- coding: utf-8 -*-
r"""
Worklog.py ‚Äî Tempo ‚Üí —Ä—É—Å—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏, –±–µ–∑ ¬´–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–õ–æ–≥–∏–Ω¬ª.
+ Summary (–û–ø–∏—Å–∞–Ω–∏–µ) –∏ Labels (–ú–µ—Ç–∫–∏) –∏–∑ Jira Issue fields.
+ FTE –ø–æ—Å—Ç—Ä–æ—á–Ω–æ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø–æ–º–µ—Å—è—á–Ω—ã–π FTE (monthly_fte + monthly_total).
+ –í–†–ï–ú–ï–ù–ù–û: –Ω–æ—Ä–º—ã —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π –∑–∞—à–∏—Ç—ã –¥–ª—è 2025 –≥–æ–¥–∞ —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∫–∞–ª–µ–Ω–¥–∞—Ä—é –†–§.
+ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è (–±–µ–∑ GroupBy.apply –∏ –±–µ–∑ FutureWarning).
+ –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Excel HYPERLINK (–±–µ–∑ f-—Å—Ç—Ä–æ–∫).
"""

import os, re, json, time, requests, pandas as pd
from datetime import datetime, timedelta, date
import concurrent.futures
from pathlib import Path
from typing import Dict, Tuple, Optional

# === canon utils (–≤–∞—à–∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ) ===
from canon_utils import load_columns_map, canonize_fact, write_schema_json, norm_text, to_mmYYYY

# ========= ENV =========
JIRA_URL     = os.getenv("JIRA_URL", "").strip()
BEARER_TOKEN = os.getenv("JIRA_TEMPO_TOKEN", "").strip()

# –û–∫–Ω–æ –≤—ã–≥—Ä—É–∑–∫–∏ –∏–∑ Tempo (—Ç–æ–ª—å–∫–æ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞; –Ω–∞ —Ä–∞—Å—á—ë—Ç FTE –Ω–µ –≤–ª–∏—è–µ—Ç)
_today = date.today()
FROM_DATE = os.getenv("FROM_DATE", f"{_today.year}-01-01")
TO_DATE   = os.getenv("TO_DATE",   f"{_today.year}-12-31")

TIMEZONE = os.getenv("TIMEZONE", "Europe/Moscow").strip()
EXCEL_AUTOFIT = os.getenv("EXCEL_AUTOFIT", "1") == "1"

# (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ê–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ Account Key –ø–æ summary –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞
FILL_ACCOUNT_ENABLED  = os.getenv("FILL_ACCOUNT_ENABLED", "1") == "1"
FILL_ACCOUNT_PROJECTS = [p.strip() for p in os.getenv("FILL_ACCOUNT_PROJECTS", "INT").split(";") if p.strip()]
FILL_ACCOUNT_PREFIX   = os.getenv("FILL_ACCOUNT_PREFIX", "INT: ")

if not JIRA_URL or not BEARER_TOKEN:
    raise SystemExit("ENV JIRA_URL –∏–ª–∏ JIRA_TEMPO_TOKEN –Ω–µ –∑–∞–¥–∞–Ω—ã")

# ========= –ü–∞–ø–∫–∏ =========
WL_DIR     = Path(r"C:\Users\zaytsev_ra2\PycharmProjects\TCO\WL")
WL_OUT_DIR = WL_DIR / "–†–µ–∑—É–ª—å—Ç–∞—Ç"
WL_OUT_DIR.mkdir(parents=True, exist_ok=True)

# ====== Rates directory config ======
RATES_FOLDER  = Path(r"C:\Users\zaytsev_ra2\PycharmProjects\–ö–æ–Ω—Ç—Ä–æ–ª—å –±—é–¥–∂–µ—Ç–∞\–°—Ç–∞–≤–∫–∏")
RATES_PATTERN = "employee_rate_directory_*.xlsx"

# ========= –í–†–ï–ú–ï–ù–ù–´–ô ¬´–ñ–Å–°–¢–ö–ò–ô¬ª –ò–°–¢–û–ß–ù–ò–ö –ù–û–†–ú (–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–∞–ª–µ–Ω–¥–∞—Ä—å –†–§, 2025) =========
EXPECTED_WORKDAYS_2025: Dict[str, int] = {
    "2025-01": 17,
    "2025-02": 20,
    "2025-03": 21,
    "2025-04": 22,
    "2025-05": 18,
    "2025-06": 19,
    "2025-07": 23,
    "2025-08": 21,
    "2025-09": 22,
    "2025-10": 23,
    "2025-11": 19,
    "2025-12": 22,
}

def workdays_for_ym(ym: str) -> int:
    """
    –í–†–ï–ú–ï–ù–ù–û: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π –¢–û–õ–¨–ö–û –¥–ª—è 2025 –∏–∑ –∑–∞—à–∏—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è.
    –ï—Å–ª–∏ –º–µ—Å—è—Ü –≤–Ω–µ 2025 ‚Äî —Å–∫—Ä–∏–ø—Ç –ø–∞–¥–∞–µ—Ç, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ ¬´—Ç–∏—Ö–∏—Ö¬ª —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π.
    """
    if not ym:
        raise SystemExit("[ERROR] workdays_for_ym: –ø—É—Å—Ç–æ–π ym")
    if ym in EXPECTED_WORKDAYS_2025:
        return EXPECTED_WORKDAYS_2025[ym]
    raise SystemExit(f"[ERROR] –ù–µ—Ç –Ω–æ—Ä–º—ã –¥–ª—è {ym}. –°–µ–π—á–∞—Å —Ä–∞–∑—Ä–µ—à—ë–Ω —Ç–æ–ª—å–∫–æ 2025 –≥–æ–¥ (—Å–º. EXPECTED_WORKDAYS_2025).")

# ========= Helpers: –≤—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫/—Ñ–∞–π–ª–æ–≤ –∏ –ø—Ä. =========
def _pick_latest_file(folder: Path, pattern: str) -> Optional[Path]:
    files = sorted(Path(folder).glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)
    return files[0] if files else None

def _pick_col(df: pd.DataFrame, candidates):
    low = {c.lower(): c for c in df.columns}
    for cand in candidates:
        cl = cand.strip().lower()
        if cl in low:
            return low[cl]
    for cand in candidates:
        cl = cand.strip().lower()
        for c in df.columns:
            if cl in c.strip().lower():
                return c
    return None

def normalize_employee_name(name: str) -> str:
    if name is None:
        return ""
    s = str(name)
    s = s.replace("\u00a0", " ").replace("\u202f", " ")
    s = re.sub(r'\s*(\[[^\]]+\]\s*)+$', '', s).strip()
    s = re.sub(r'\s+', ' ', s)
    return s

def load_rate_directory_or_none(folder: Path, pattern: str) -> Optional[pd.DataFrame]:
    path = _pick_latest_file(folder, pattern)
    if path is None:
        print(f"[RATES] –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ '{folder}' –ø–æ –º–∞—Å–∫–µ '{pattern}' ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
        return None
    try:
        rdf = pd.read_excel(path)
    except Exception as e:
        print(f"[RATES] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è '{path}': {e} ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
        return None
    emp  = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–§–ò–û","–°–æ—Ç—Ä—É–¥–Ω–∏–∫ –§–ò–û"])
    rate = _pick_col(rdf, ["–°—Ç–∞–≤–∫–∞","–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á","–°—Ç–∞–≤–∫–∞ (‚ÇΩ/—á)","–°—Ç–æ–∏–º–æ—Å—Ç—å —á–∞—Å–∞","–¶–µ–Ω–∞ —á–∞—Å–∞"])
    cc   = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö","–¶–ö","Cost Center","–°–¶–ö"])
    dept = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ","–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ","–û—Ç–¥–µ–ª","–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç"])
    cat  = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è","–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞","–ö–∞—Ç–µ–≥–æ—Ä–∏—è"])
    if not emp or not rate:
        print(f"[RATES] –í '{path.name}' –Ω–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–°–æ—Ç—Ä—É–¥–Ω–∏–∫/–°—Ç–∞–≤–∫–∞) ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
        return None
    rename = {emp:"–°–æ—Ç—Ä—É–¥–Ω–∏–∫", rate:"–°—Ç–∞–≤–∫–∞"}
    if cc:   rename[cc]   = "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö"
    if dept: rename[dept] = "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"
    if cat:  rename[cat]  = "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è"
    rdf = rdf.rename(columns=rename)
    rdf["–°—Ç–∞–≤–∫–∞"] = pd.to_numeric(rdf["–°—Ç–∞–≤–∫–∞"], errors="coerce")
    rdf["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] = rdf["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].apply(normalize_employee_name)
    keep = [c for c in ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–°—Ç–∞–≤–∫–∞","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è","–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] if c in rdf.columns]
    rdf = rdf[keep].dropna(subset=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"]).drop_duplicates(subset=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"])
    print(f"[RATES] –ó–∞–≥—Ä—É–∂–µ–Ω —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ '{path.name}': {len(rdf)} —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤")
    return rdf

def enrich_with_rates(df: pd.DataFrame, rates: Optional[pd.DataFrame]) -> pd.DataFrame:
    if rates is None or rates.empty:
        return df
    if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫" not in df.columns:
        emp = _pick_col(df, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–§–ò–û","–°–æ—Ç—Ä—É–¥–Ω–∏–∫ –§–ò–û"])
        if not emp:
            print("[RATES] –ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ '–°–æ—Ç—Ä—É–¥–Ω–∏–∫' ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
            return df
        if emp != "–°–æ—Ç—Ä—É–¥–Ω–∏–∫":
            df = df.rename(columns={emp:"–°–æ—Ç—Ä—É–¥–Ω–∏–∫"})
    df["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] = df["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].apply(normalize_employee_name)
    if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm" not in rates.columns:
        rates["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] = rates["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].apply(normalize_employee_name)
    merged = df.merge(rates, on="–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm", how="left", suffixes=("", "_rate"))
    if "–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" in merged.columns and "–°—Ç–∞–≤–∫–∞" not in merged.columns:
        merged = merged.rename(columns={"–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á": "–°—Ç–∞–≤–∫–∞"})
    hours_col = _pick_col(merged, ["–ß–∞—Å—ã —Ñ–∞–∫—Ç","–ß–∞—Å—ã","Hours","–í—Ä–µ–º—è, —á"])
    if hours_col:
        rate_col = "–°—Ç–∞–≤–∫–∞" if "–°—Ç–∞–≤–∫–∞" in merged.columns else ("–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" if "–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" in merged.columns else None)
        if rate_col:
            merged["–°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ"] = (merged[hours_col].fillna(0) * merged[rate_col].fillna(0)).round(2)
    if "–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" in merged.columns and "–°—Ç–∞–≤–∫–∞" in merged.columns:
        merged = merged.drop(columns=["–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á"])
    for c in ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_rate","–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"]:
        if c in merged.columns:
            merged = merged.drop(columns=[c])
    return merged

FINAL_ORDER = ["–¢–∏–ø –∑–∞–¥–∞—á–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É", "–û–ø–∏—Å–∞–Ω–∏–µ", "–ú–µ—Ç–∫–∏", "–ü—Ä–æ–µ–∫—Ç", "–°–µ—Ä–≤–∏—Å", "–ú–µ—Å—è—Ü",
               "–°–æ—Ç—Ä—É–¥–Ω–∏–∫", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
               "–¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "–ß–∞—Å—ã —Ñ–∞–∫—Ç", "FTE —Ñ–∞–∫—Ç", "–°—Ç–∞–≤–∫–∞", "–°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ"]

def reorder_final(df: pd.DataFrame) -> pd.DataFrame:
    keep = [c for c in FINAL_ORDER if c in df.columns]
    rest = [c for c in df.columns if c not in keep]
    return df[keep + rest]

COLUMNS_MAP_PATH = Path(__file__).with_name("columns_map.json")

HEADERS = {
    "Authorization": f"Bearer {BEARER_TOKEN}",
    "Content-Type": "application/json",
    "Accept": "application/json"
}

# ========= –£–º–Ω—ã–π –∞–≤—Ç–æ—Ñ–∏—Ç =========
WIDTH_OVERRIDES = {"–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É": 16, "–°–µ—Ä–≤–∏—Å": 34, "–û–ø–∏—Å–∞–Ω–∏–µ": 60}
CLAMP_MIN, CLAMP_MAX = 6, 60

def _visible_text(cell_val):
    if not isinstance(cell_val, str):
        return "" if cell_val is None else str(cell_val)
    s = cell_val.strip()
    if not s.startswith("="):
        return s
    m = re.match(r'^\s*=\s*HYPERLINK\s*\(\s*"[^"]*"\s*[,;]\s*"([^"]*)"\s*\)\s*$', s, re.IGNORECASE)
    return m.group(1) if m else s

def excel_autofit_smart(path: Path):
    try:
        from openpyxl import load_workbook
        wb = load_workbook(path)
        for ws in wb.worksheets:
            headers = {cell.column: (cell.value or "") for cell in ws[1]}
            for col in ws.columns:
                col_idx = col[0].column
                head = str(headers.get(col_idx, "")).strip()
                if head in WIDTH_OVERRIDES:
                    ws.column_dimensions[col[0].column_letter].width = WIDTH_OVERRIDES[head]
                    continue
                max_len = max(len(_visible_text(c.value)) for c in col)
                max_len = max(max_len, len(head))
                ws.column_dimensions[col[0].column_letter].width = max(CLAMP_MIN, min(max_len + 2, CLAMP_MAX))
        wb.save(path)
    except Exception as e:
        print(f"[autofit] –ø—Ä–æ–ø—É—â–µ–Ω–æ: {e}")

# ========= JSON helpers =========
def load_json(path: Path) -> Dict[str, str]:
    if path.exists():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_json(path: Path, data: Dict[str, str]):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
    except Exception as e:
        print(f"[cache] –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {path.name}: {e}")

# ========= Jira/Tempo helpers =========
def get_display_name(wid: str) -> Tuple[str, str]:
    url = "{}/rest/api/2/user?key={}".format(JIRA_URL, wid)
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        if r.status_code == 200:
            user = r.json()
            return wid, user.get("displayName", wid)
    except Exception:
        time.sleep(0.2)
    return wid, wid

def fetch_issue_summary_labels(issue_key: str) -> Tuple[str, Dict[str, str]]:
    url = "{}/rest/api/2/issue/{}?fields=summary,labels".format(JIRA_URL, issue_key)
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        if r.status_code == 200:
            data = r.json() or {}
            fields = data.get("fields") or {}
            return issue_key, {
                "summary": fields.get("summary", "") or "",
                "labels": ",".join(fields.get("labels") or [])
            }
    except Exception:
        time.sleep(0.2)
    return issue_key, {"summary": "", "labels": ""}

def sanitize_for_account(title: str) -> str:
    if title is None:
        return ""
    s = " ".join(str(title).replace("\n", " ").replace("\r", " ").split())
    return s[:100]

def hyperlink_formula(jira_base_url: str, key: str) -> str:
    """Excel HYPERLINK –±–µ–∑ f-—Å—Ç—Ä–æ–∫ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ)."""
    if not key:
        return ""
    k = str(key).strip().replace('"', '""')
    base = str(jira_base_url).rstrip('/')
    return '=HYPERLINK("{}", "{}")'.format("{}/browse/{}".format(base, k), k)

# ========= Tempo fetch =========
tempo_url = "{}/rest/tempo-timesheets/4/worklogs/search".format(JIRA_URL)
payload = {"from": FROM_DATE, "to": TO_DATE}
resp = requests.post(tempo_url, json=payload, headers=HEADERS, timeout=120)
if resp.status_code != 200:
    print("–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Tempo:", resp.status_code, resp.text); raise SystemExit(1)
worklogs = resp.json() or []

# ========= Users cache =========
worker_ids = list({w.get("worker") for w in worklogs if w.get("worker")})
worker_map_path = WL_DIR / 'worker_map.json'
worker_map = load_json(worker_map_path)
new_ids = [wid for wid in worker_ids if wid and wid not in worker_map]
if new_ids:
    print("–ù–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {} ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à...".format(len(new_ids)))
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as ex:
        for wid, name in ex.map(get_display_name, new_ids):
            worker_map[wid] = name
    save_json(worker_map_path, worker_map)

# ========= Summary/labels cache =========
summary_labels_cache_path = WL_DIR / "issue_summary_labels_cache.json"
summary_labels_cache = load_json(summary_labels_cache_path)

all_issue_keys = sorted({(w.get("issue") or {}).get("key") for w in worklogs if (w.get("issue") or {}).get("key")})
need_keys_sl = [k for k in all_issue_keys if k and k not in summary_labels_cache]
if need_keys_sl:
    print("–ù–æ–≤—ã—Ö issues –¥–ª—è summary+labels: {}".format(len(need_keys_sl)))
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as ex:
        for ikey, data in ex.map(fetch_issue_summary_labels, need_keys_sl):
            summary_labels_cache[ikey] = data
    save_json(summary_labels_cache_path, summary_labels_cache)

# ========= Rows build =========
def parse_started_ts(value) -> Optional[datetime]:
    """Tempo started (UTC aware) ‚Üí –ª–æ–∫–∞–ª—å–Ω–æ–µ –Ω–∞–∏–≤–Ω–æ–µ –≤—Ä–µ–º—è (TIMEZONE)."""
    try:
        ts = pd.to_datetime(value, utc=True, errors="coerce")
        if pd.isna(ts):
            return None
        ts = ts.tz_convert(TIMEZONE).tz_localize(None)
        return ts.to_pydatetime()
    except Exception:
        return None

rows = []
filled_cnt = 0

for w in worklogs:
    issue = w.get("issue") or {}
    attrs = w.get("attributes") or {}
    activity_type = (attrs.get("_–¢–∏–ø–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏_", {}) or {}).get("value")

    started_dt = parse_started_ts(w.get("started"))
    ym = started_dt.strftime("%Y-%m") if started_dt else None
    mmYYYY = started_dt.strftime("%m/%Y") if started_dt else None

    hours = (w.get("timeSpentSeconds") or 0) / 3600.0

    # FTE –ø–æ—Å—Ç—Ä–æ—á–Ω–æ —Å—Ç—Ä–æ–≥–æ –æ—Ç —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π –§–ê–ö–¢–ò–ß–ï–°–ö–û–ì–û –º–µ—Å—è—Ü–∞ (–∑–∞—à–∏—Ç—ã–µ –Ω–æ—Ä–º—ã 2025):
    days_norm = workdays_for_ym(ym) if ym else None
    if days_norm is None:
        fte = None
    else:
        fte = (hours / (8.0 * days_norm)) if days_norm else None

    issue_key   = (issue.get("key") or "").strip()
    project_key = issue.get("projectKey")
    issue_type  = issue.get("issueType")
    account_key = issue.get("accountKey")

    sl = summary_labels_cache.get(issue_key) or {"summary": "", "labels": ""}
    summary_for_report = sl.get("summary", "") or ""
    labels_for_report  = sl.get("labels", "") or ""

    if FILL_ACCOUNT_ENABLED and project_key in FILL_ACCOUNT_PROJECTS and not str(account_key or "").strip():
        smry = summary_for_report or issue_key
        account_key = "{}{}".format(FILL_ACCOUNT_PREFIX, sanitize_for_account(smry)) if FILL_ACCOUNT_PREFIX else sanitize_for_account(smry)
        filled_cnt += 1

    rows.append({
        "IssueKey_tmp": issue_key,
        "Worker Name":  worker_map.get(w.get("worker"), w.get("worker")),
        "Account Key":  account_key,
        "Project Key":  project_key,
        "Issue Type":   issue_type,
        "Activity Type":activity_type,
        "Started (mm/yyyy)": mmYYYY,
        "Time Spent (h)": round(hours, 2),
        "FTE":                round(fte, 3) if fte is not None else None,
        "Summary":            summary_for_report,
        "Labels":             labels_for_report,
        "_ym":                ym,  # –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    })

print("–ê–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ Account Key –∏–∑ summary: {} –∑–∞–¥–∞—á.".format(filled_cnt))

df = pd.DataFrame(rows)

# ========= RU + —Å—Å—ã–ª–∫–∞ =========
def hyperlink_formula_safe(jira_base_url: str, key: str) -> str:
    if not key:
        return ""
    k = str(key).strip().replace('"', '""')
    base = str(jira_base_url).rstrip('/')
    return '=HYPERLINK("{}", "{}")'.format("{}/browse/{}".format(base, k), k)

df.insert(0, "–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É", df["IssueKey_tmp"].map(lambda k: hyperlink_formula_safe(JIRA_URL, k)))
df.drop(columns=["IssueKey_tmp"], inplace=True)
df.rename(columns={
    "Worker Name":        "–°–æ—Ç—Ä—É–¥–Ω–∏–∫",
    "Account Key":        "–°–µ—Ä–≤–∏—Å",
    "Project Key":        "–ü—Ä–æ–µ–∫—Ç",
    "Issue Type":         "–¢–∏–ø –∑–∞–¥–∞—á–∏",
    "Activity Type":      "–¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
    "Started (mm/yyyy)":  "–ú–µ—Å—è—Ü",
    "Time Spent (h)":     "–ß–∞—Å—ã —Ñ–∞–∫—Ç",
    "FTE":                "FTE —Ñ–∞–∫—Ç",
    "Summary":            "–û–ø–∏—Å–∞–Ω–∏–µ",
    "Labels":             "–ú–µ—Ç–∫–∏",
}, inplace=True)

RU_ORDER = ["–¢–∏–ø –∑–∞–¥–∞—á–∏","–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É","–û–ø–∏—Å–∞–Ω–∏–µ","–ú–µ—Ç–∫–∏","–ü—Ä–æ–µ–∫—Ç","–°–µ—Ä–≤–∏—Å","–ú–µ—Å—è—Ü",
            "–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
            "–¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏","–ß–∞—Å—ã —Ñ–∞–∫—Ç","FTE —Ñ–∞–∫—Ç","–°—Ç–∞–≤–∫–∞","–°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ"]
df = df[[c for c in RU_ORDER if c in df.columns] + [c for c in df.columns if c not in RU_ORDER]]

# ====== –°—Ç–∞–≤–∫–∏ ======
try:
    _rates_df = load_rate_directory_or_none(RATES_FOLDER, RATES_PATTERN)
    df = enrich_with_rates(df, _rates_df)
    df = reorder_final(df)
except Exception as _e:
    print("[RATES] –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Å—Ç–∞–≤–∫–∞–º–∏: {}".format(_e))

# ========= –ê–≥—Ä–µ–≥–∞—Ü–∏—è (–≤–µ–∫—Ç–æ—Ä–Ω–æ) =========
# —Ç–µ—Ö. –∫–ª—é—á –º–µ—Å—è—Ü–∞
if "_ym" not in df.columns:
    df["_ym"] = pd.to_datetime(df["–ú–µ—Å—è—Ü"], format="%m/%Y", errors="coerce").dt.strftime("%Y-%m")

# 1) –ø–æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫—É –∏ –º–µ—Å—è—Ü—É
monthly = (
    df.groupby(["_ym", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫"], dropna=False)["–ß–∞—Å—ã —Ñ–∞–∫—Ç"]
      .sum()
      .reset_index()
      .rename(columns={"_ym": "–ú–µ—Å—è—Ü (YYYY-MM)"})
)
monthly["–ù–æ—Ä–º–∞_–¥–Ω–µ–π"]  = monthly["–ú–µ—Å—è—Ü (YYYY-MM)"].map(workdays_for_ym).astype("Int64")
monthly["–ù–æ—Ä–º–∞_—á–∞—Å–æ–≤"] = (8 * monthly["–ù–æ—Ä–º–∞_–¥–Ω–µ–π"]).astype("Int64")
monthly["FTE –º–µ—Å—è—Ü"]   = (monthly["–ß–∞—Å—ã —Ñ–∞–∫—Ç"] / monthly["–ù–æ—Ä–º–∞_—á–∞—Å–æ–≤"]).round(3)

# 2) –∏—Ç–æ–≥–æ –ø–æ –º–µ—Å—è—Ü—É
monthly_total = (
    monthly.groupby(["–ú–µ—Å—è—Ü (YYYY-MM)"])["–ß–∞—Å—ã —Ñ–∞–∫—Ç"]
           .sum()
           .reset_index()
)
monthly_total["–ù–æ—Ä–º–∞_–¥–Ω–µ–π"]  = monthly_total["–ú–µ—Å—è—Ü (YYYY-MM)"].map(workdays_for_ym).astype("Int64")
monthly_total["–ù–æ—Ä–º–∞_—á–∞—Å–æ–≤"] = (8 * monthly_total["–ù–æ—Ä–º–∞_–¥–Ω–µ–π"]).astype("Int64")
monthly_total["FTE –º–µ—Å—è—Ü (–∏—Ç–æ–≥–æ)"] = (monthly_total["–ß–∞—Å—ã —Ñ–∞–∫—Ç"] / monthly_total["–ù–æ—Ä–º–∞_—á–∞—Å–æ–≤"]).round(3)

# ========= Excel =========
stamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
raw_path = WL_DIR / "worklogs_{}.xlsx".format(stamp)
with pd.ExcelWriter(raw_path, engine="openpyxl") as w:
    df.to_excel(w, index=False, sheet_name="worklogs_raw")
    monthly.to_excel(w, index=False, sheet_name="monthly_fte")
    monthly_total.to_excel(w, index=False, sheet_name="monthly_total")
if EXCEL_AUTOFIT:
    excel_autofit_smart(raw_path)
print("‚úÖ RAW: {}".format(raw_path))

# ========= Canonical =========
columns_map = load_columns_map(COLUMNS_MAP_PATH)
df_can, canon_log = canonize_fact(df, columns_map, coerce_types=True)
if "–ú–µ—Å—è—Ü" in df_can.columns:     df_can["–ú–µ—Å—è—Ü"] = df_can["–ú–µ—Å—è—Ü"].map(to_mmYYYY)
if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫" in df_can.columns: df_can["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"] = df_can["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].map(norm_text)
if "–°–µ—Ä–≤–∏—Å" in df_can.columns:    df_can["–°–µ—Ä–≤–∏—Å"] = df_can["–°–µ—Ä–≤–∏—Å"].map(norm_text)
if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–õ–æ–≥–∏–Ω" in df_can.columns:
    df_can.drop(columns=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–õ–æ–≥–∏–Ω"], inplace=True, errors="ignore")
df_can = df_can[[c for c in RU_ORDER if c in df_can.columns] + [c for c in df_can.columns if c not in RU_ORDER]]

can_path = WL_OUT_DIR / "worklogs_canonical_{}.xlsx".format(stamp)
with pd.ExcelWriter(can_path, engine="openpyxl") as w:
    df_can.to_excel(w, index=False, sheet_name="worklogs")
    monthly.to_excel(w, index=False, sheet_name="monthly_fte")
    monthly_total.to_excel(w, index=False, sheet_name="monthly_total")
    if not canon_log.empty:
        canon_log.to_excel(w, index=False, sheet_name="canonical_log")
if EXCEL_AUTOFIT:
    excel_autofit_smart(can_path)

write_schema_json(can_path.with_suffix(".schema.json"),
                  {"worklogs": df_can, "monthly_fte": monthly, "monthly_total": monthly_total})
print("‚úÖ Canonical: {}".format(can_path))
print("üßæ Schema:    {}".format(can_path.with_suffix(".schema.json")))

# ========= –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (–ø—Ä–∏–º–µ—Ä: –º–∞–π 2025) =========
try:
    ym_check = "2025-05"
    may_hours_total = float(monthly_total.loc[monthly_total["–ú–µ—Å—è—Ü (YYYY-MM)"].eq(ym_check), "–ß–∞—Å—ã —Ñ–∞–∫—Ç"].sum())
    rdays = workdays_for_ym(ym_check)
    denom = 8 * rdays
    if denom:
        print("[CHECK] –ú–ê–ô 2025: Œ£—á–∞—Å—ã={}, —Ä–∞–±_–¥–Ω–µ–π={}, –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å={}, FTE={:.3f}".format(
            may_hours_total, rdays, denom, (may_hours_total/denom if denom else 0)))
except Exception as e:
    print("[CHECK] –û—à–∏–±–∫–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –º–∞—è: {}".format(e))
