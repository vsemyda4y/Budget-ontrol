# -*- coding: utf-8 -*-
r"""
Worklog.py ‚Äî Tempo ‚Üí —Ä—É—Å—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏, –±–µ–∑ ¬´–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–õ–æ–≥–∏–Ω¬ª.
+ Summary (–û–ø–∏—Å–∞–Ω–∏–µ) –∏ Labels (–ú–µ—Ç–∫–∏) –∏–∑ Jira Issue fields.

–ü–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ (–¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏):
  –¢–∏–ø –∑–∞–¥–∞—á–∏, –°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É, –û–ø–∏—Å–∞–Ω–∏–µ, –ú–µ—Ç–∫–∏, –ü—Ä–æ–µ–∫—Ç, –°–µ—Ä–≤–∏—Å, –ú–µ—Å—è—Ü,
  –°–æ—Ç—Ä—É–¥–Ω–∏–∫, –°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è, –°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö, –°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ,
  –¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, –ß–∞—Å—ã —Ñ–∞–∫—Ç, FTE —Ñ–∞–∫—Ç, –°—Ç–∞–≤–∫–∞, –°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ
"""

import os, re, json, time, requests, pandas as pd
from workalendar.europe import Russia
from datetime import datetime, timedelta
import concurrent.futures
from pathlib import Path
from typing import Dict, Tuple, List, Optional

from canon_utils import load_columns_map, canonize_fact, write_schema_json, norm_text, to_mmYYYY

# ========= ENV =========
JIRA_URL     = os.getenv
BEARER_TOKEN = os.getenv
YEAR         = int(os.getenv("YEAR", datetime.now().year))
if not JIRA_URL or not BEARER_TOKEN:
    raise SystemExit("ENV JIRA_URL/JIRA_TEMPO_TOKEN –Ω–µ –∑–∞–¥–∞–Ω—ã")

FILL_ACCOUNT_ENABLED  = os.getenv("FILL_ACCOUNT_ENABLED", "1") == "1"
FILL_ACCOUNT_PROJECTS = [p.strip() for p in os.getenv("FILL_ACCOUNT_PROJECTS", "INT").split(";") if p.strip()]
FILL_ACCOUNT_PREFIX   = os.getenv("FILL_ACCOUNT_PREFIX", "INT: ")
EXCEL_AUTOFIT         = os.getenv("EXCEL_AUTOFIT", "1") == "1"  # —É–º–Ω—ã–π –∞–≤—Ç–æ—Ñ–∏—Ç

WL_DIR     = Path(r"C:\Users\zaytsev_ra2\PycharmProjects\TCO\WL")
WL_OUT_DIR = WL_DIR / "–†–µ–∑—É–ª—å—Ç–∞—Ç"

# ====== Rates directory config (added) ======
RATES_FOLDER  = Path(r"C:\Users\zaytsev_ra2\PycharmProjects\–ö–æ–Ω—Ç—Ä–æ–ª—å –±—é–¥–∂–µ—Ç–∞\–°—Ç–∞–≤–∫–∏")
RATES_PATTERN = "employee_rate_directory_*.xlsx"  # mask for the rates files

def _pick_latest_file(folder: Path, pattern: str) -> Optional[Path]:
    files = sorted(Path(folder).glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)
    return files[0] if files else None

def _pick_col(df: pd.DataFrame, candidates):
    low = {c.lower(): c for c in df.columns}
    for cand in candidates:
        cl = cand.strip().lower()
        if cl in low:
            return low[cl]
    for cand in candidates:
        cl = cand.strip().lower()
        for c in df.columns:
            if cl in c.strip().lower():
                return c
    return None

def normalize_employee_name(name: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏–º—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ –¥–ª—è –¥–∂–æ–π–Ω–∞:
    - —É–±–∏—Ä–∞–µ—Ç —Å—É—Ñ—Ñ–∏–∫—Å—ã –≤–∏–¥–∞ ' [X]' –∏–ª–∏ –ª—é–±—ã–µ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ —Å–∫–æ–±–∫–∏ –≤ –∫–æ–Ω—Ü–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ)
    - —É–±–∏—Ä–∞–µ—Ç –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –ø–æ –∫—Ä–∞—è–º –∏ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏
    """
    if name is None:
        return ""
    s = str(name)
    # –∑–∞–º–µ–Ω–∏—Ç—å –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –∏ —É–∑–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–±—ã—á–Ω—ã–µ
    s = s.replace("\u00a0", " ").replace("\u202f", " ")
    # —É–±—Ä–∞—Ç—å –≤—Å–µ –≥—Ä—É–ø–ø—ã [...] –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏ (–æ–¥–Ω—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ)
    s = re.sub(r'\s*(\[[^\]]+\]\s*)+$', '', s).strip()
    # —Å—Ö–ª–æ–ø–Ω—É—Ç—å –º—É–ª—å—Ç–∏-–ø—Ä–æ–±–µ–ª—ã
    s = re.sub(r'\s+', ' ', s)
    return s

def load_rate_directory_or_none(folder: Path, pattern: str) -> Optional[pd.DataFrame]:
    path = _pick_latest_file(folder, pattern)
    if path is None:
        print(f"[RATES] –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ '{folder}' –ø–æ –º–∞—Å–∫–µ '{pattern}' ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
        return None
    try:
        rdf = pd.read_excel(path)
    except Exception as e:
        print(f"[RATES] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è '{path}': {e} ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
        return None
    emp  = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–§–ò–û","–°–æ—Ç—Ä—É–¥–Ω–∏–∫ –§–ò–û"])
    rate = _pick_col(rdf, ["–°—Ç–∞–≤–∫–∞","–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á","–°—Ç–∞–≤–∫–∞ (‚ÇΩ/—á)","–°—Ç–æ–∏–º–æ—Å—Ç—å —á–∞—Å–∞","–¶–µ–Ω–∞ —á–∞—Å–∞"])
    cc   = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö","–¶–ö","Cost Center","–°–¶–ö"])
    dept = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ","–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ","–û—Ç–¥–µ–ª","–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç"])
    cat  = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è","–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞","–ö–∞—Ç–µ–≥–æ—Ä–∏—è"])
    if not emp or not rate:
        print(f"[RATES] –í '{path.name}' –Ω–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–°–æ—Ç—Ä—É–¥–Ω–∏–∫/–°—Ç–∞–≤–∫–∞) ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
        return None
    rename = {emp:"–°–æ—Ç—Ä—É–¥–Ω–∏–∫", rate:"–°—Ç–∞–≤–∫–∞"}
    if cc:   rename[cc]   = "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö"
    if dept: rename[dept] = "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"
    if cat:  rename[cat]  = "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è"
    rdf = rdf.rename(columns=rename)
    rdf["–°—Ç–∞–≤–∫–∞"] = pd.to_numeric(rdf["–°—Ç–∞–≤–∫–∞"], errors="coerce")

    # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–ª—é—á + –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –Ω–µ–º—É
    rdf["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] = rdf["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].apply(normalize_employee_name)
    keep = [c for c in ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–°—Ç–∞–≤–∫–∞","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è","–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] if c in rdf.columns]
    rdf = rdf[keep].dropna(subset=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"]).drop_duplicates(subset=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"])
    print(f"[RATES] –ó–∞–≥—Ä—É–∂–µ–Ω —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ '{path.name}': {len(rdf)} —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤")
    return rdf

def enrich_with_rates(df: pd.DataFrame, rates: Optional[pd.DataFrame]) -> pd.DataFrame:
    if rates is None or rates.empty:
        return df
    if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫" not in df.columns:
        emp = _pick_col(df, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–§–ò–û","–°–æ—Ç—Ä—É–¥–Ω–∏–∫ –§–ò–û"])
        if not emp:
            print("[RATES] –ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ '–°–æ—Ç—Ä—É–¥–Ω–∏–∫' –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
            return df
        if emp != "–°–æ—Ç—Ä—É–¥–Ω–∏–∫":
            df = df.rename(columns={emp:"–°–æ—Ç—Ä—É–¥–Ω–∏–∫"})
    # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–ª—é—á –¥–ª—è –æ–±–µ–∏—Ö —Ç–∞–±–ª–∏—Ü
    df["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] = df["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].apply(normalize_employee_name)
    if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm" not in rates.columns:
        rates["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] = rates["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].apply(normalize_employee_name)
    merged = df.merge(rates, on="–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm", how="left", suffixes=("", "_rate"))

    # —Å—Ç–∞–≤–∫–∞ –≤ –æ—Ç—á—ë—Ç–µ –ø–æ–¥ –∏–º–µ–Ω–µ–º "–°—Ç–∞–≤–∫–∞"
    if "–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" in merged.columns and "–°—Ç–∞–≤–∫–∞" not in merged.columns:
        merged = merged.rename(columns={"–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á": "–°—Ç–∞–≤–∫–∞"})
    elif "–°—Ç–∞–≤–∫–∞" not in merged.columns and "–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" not in merged.columns:
        # –µ—Å–ª–∏ –Ω–∏ –æ–¥–Ω–æ–π –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å raw "–°—Ç–∞–≤–∫–∞" –∏–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
        pass

    # —Å—Ç–æ–∏–º–æ—Å—Ç—å = —á–∞—Å—ã * —Å—Ç–∞–≤–∫–∞
    hours_col = _pick_col(merged, ["–ß–∞—Å—ã —Ñ–∞–∫—Ç","–ß–∞—Å—ã","Hours","–í—Ä–µ–º—è, —á"])
    if hours_col:
        # –Ω–∞–π–¥—ë–º –∫–æ–ª–æ–Ω–∫—É —Å—Ç–∞–≤–∫–∏
        rate_col = "–°—Ç–∞–≤–∫–∞" if "–°—Ç–∞–≤–∫–∞" in merged.columns else ("–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" if "–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" in merged.columns else None)
        if rate_col:
            merged["–°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ"] = (merged[hours_col].fillna(0) * merged[rate_col].fillna(0)).round(2)
    else:
        print("[RATES] –ù–µ –Ω–∞—à—ë–ª –∫–æ–ª–æ–Ω–∫—É —á–∞—Å–æ–≤ ‚Äî –Ω–µ —Å—á–∏—Ç–∞—é '–°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ'.")

    # —É–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ/—Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    if "–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" in merged.columns and "–°—Ç–∞–≤–∫–∞" in merged.columns:
        merged = merged.drop(columns=["–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á"])
    if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫_rate" in merged.columns:
        merged = merged.drop(columns=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_rate"])
    if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm" in merged.columns:
        merged = merged.drop(columns=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"])

    return merged

# ====== Final column order helper ======
FINAL_ORDER = ["–¢–∏–ø –∑–∞–¥–∞—á–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É", "–û–ø–∏—Å–∞–Ω–∏–µ", "–ú–µ—Ç–∫–∏", "–ü—Ä–æ–µ–∫—Ç", "–°–µ—Ä–≤–∏—Å", "–ú–µ—Å—è—Ü",
               "–°–æ—Ç—Ä—É–¥–Ω–∏–∫", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
               "–¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "–ß–∞—Å—ã —Ñ–∞–∫—Ç", "FTE —Ñ–∞–∫—Ç", "–°—Ç–∞–≤–∫–∞", "–°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ"]

def reorder_final(df: pd.DataFrame) -> pd.DataFrame:
    keep = [c for c in FINAL_ORDER if c in df.columns]
    rest = [c for c in df.columns if c not in keep]
    return df[keep + rest]
# ============================================
WL_OUT_DIR.mkdir(parents=True, exist_ok=True)

COLUMNS_MAP_PATH = Path(__file__).with_name("columns_map.json")

HEADERS = {
    "Authorization": f"Bearer {BEARER_TOKEN}",
    "Content-Type": "application/json",
    "Accept": "application/json"
}

# ========= –£–º–Ω—ã–π –∞–≤—Ç–æ—Ñ–∏—Ç =========
WIDTH_OVERRIDES = {
    "–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É": 16,
    "–°–µ—Ä–≤–∏—Å": 34,
    "–û–ø–∏—Å–∞–Ω–∏–µ": 60,
}
CLAMP_MIN, CLAMP_MAX = 6, 60

def _visible_text(cell_val):
    if not isinstance(cell_val, str):
        return "" if cell_val is None else str(cell_val)
    s = cell_val.strip()
    if not s.startswith("="):
        return s
    m = re.match(r'^\s*=\s*HYPERLINK\s*\(\s*"[^"]*"\s*[,;]\s*"([^"]*)"\s*\)\s*$', s, re.IGNORECASE)
    return m.group(1) if m else s

def excel_autofit_smart(path: Path):
    try:
        from openpyxl import load_workbook
        wb = load_workbook(path)
        for ws in wb.worksheets:
            headers = {cell.column: (cell.value or "") for cell in ws[1]}
            for col in ws.columns:
                col_idx = col[0].column
                head = str(headers.get(col_idx, "")).strip()
                if head in WIDTH_OVERRIDES:
                    ws.column_dimensions[col[0].column_letter].width = WIDTH_OVERRIDES[head]
                    continue
                max_len = max(len(_visible_text(c.value)) for c in col)
                max_len = max(max_len, len(head))
                ws.column_dimensions[col[0].column_letter].width = max(CLAMP_MIN, min(max_len + 2, CLAMP_MAX))
        wb.save(path)
    except Exception as e:
        print(f"[autofit] –ø—Ä–æ–ø—É—â–µ–Ω–æ: {e}")

# ========= helpers =========
def load_json(path: Path) -> Dict[str, str]:
    if path.exists():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_json(path: Path, data: Dict[str, str]):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
    except Exception as e:
        print(f"[cache] –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {path.name}: {e}")

def get_display_name(wid: str) -> Tuple[str, str]:
    url = f"{JIRA_URL}/rest/api/2/user?key={wid}"
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        if r.status_code == 200:
            user = r.json()
            return wid, user.get("displayName", wid)
    except Exception:
        time.sleep(0.2)
    return wid, wid

def fetch_issue_summary(issue_key: str) -> Tuple[str, str]:
    """–°—Ç–∞—Ä—ã–π –ª—ë–≥–∫–∏–π —Ñ–µ—Ç—á–µ—Ä summary ‚Äî –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–∏ Account Key)."""
    try:
        url = f"{JIRA_URL}/rest/api/2/issue/{issue_key}?fields=summary"
        r = requests.get(url, headers=HEADERS, timeout=10)
        if r.status_code == 200:
            data = r.json() or {}
            smry = (data.get("fields") or {}).get("summary")
            if smry:
                return issue_key, str(smry)
    except Exception:
        time.sleep(0.2)
    return issue_key, issue_key

def fetch_issue_summary_labels(issue_key: str) -> Tuple[str, Dict[str, str]]:
    """–§–µ—Ç—á–µ—Ä summary+labels –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –û–ø–∏—Å–∞–Ω–∏—è –∏ –ú–µ—Ç–æ–∫ –≤ –æ—Ç—á—ë—Ç–µ."""
    url = f"{JIRA_URL}/rest/api/2/issue/{issue_key}?fields=summary,labels"
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        if r.status_code == 200:
            data = r.json() or {}
            fields = data.get("fields") or {}
            return issue_key, {
                "summary": fields.get("summary", "") or "",
                "labels": ",".join(fields.get("labels") or [])
            }
    except Exception:
        time.sleep(0.2)
    return issue_key, {"summary": "", "labels": ""}

def sanitize_for_account(title: str) -> str:
    if title is None:
        return ""
    s = " ".join(str(title).replace("\n", " ").replace("\r", " ").split())
    return s[:100]

def hyperlink_formula(jira_base_url: str, key: str) -> str:
    if not key:
        return ""
    k = str(key).strip().replace('"', '""')
    return f'=HYPERLINK("{jira_base_url.rstrip("/")}/browse/{k}","{k}")'

# ========= 1) Tempo =========
tempo_url = f"{JIRA_URL}/rest/tempo-timesheets/4/worklogs/search"
payload = {"from": f"{YEAR}-01-01", "to": f"{YEAR}-12-31"}
resp = requests.post(tempo_url, json=payload, headers=HEADERS, timeout=120)
if resp.status_code != 200:
    print("–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Tempo:", resp.status_code, resp.text); raise SystemExit(1)
worklogs = resp.json() or []

# ========= 2) –ö–µ—à –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (–¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–≥–æ –∏–º–µ–Ω–∏) =========
worker_ids = list({w.get("worker") for w in worklogs if w.get("worker")})
worker_map_path = WL_DIR / 'worker_map.json'
worker_map = load_json(worker_map_path)
new_ids = [wid for wid in worker_ids if wid and wid not in worker_map]
if new_ids:
    print(f"–ù–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(new_ids)} ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as ex:
        for wid, name in ex.map(get_display_name, new_ids):
            worker_map[wid] = name
    save_json(worker_map_path, worker_map)

# ========= 3) –ö–µ—à summary –∑–∞–¥–∞—á (–¥–ª—è –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è Account Key) =========
summary_cache_path = WL_DIR / "issue_summary_cache.json"
summary_cache = load_json(summary_cache_path)

# ========= 3b) –ö–µ—à summary+labels –¥–ª—è –æ—Ç—á—ë—Ç–∞ =========
summary_labels_cache_path = WL_DIR / "issue_summary_labels_cache.json"
summary_labels_cache = load_json(summary_labels_cache_path)

all_issue_keys = sorted({(w.get("issue") or {}).get("key") for w in worklogs if (w.get("issue") or {}).get("key")})
need_keys_sl = [k for k in all_issue_keys if k and k not in summary_labels_cache]
if need_keys_sl:
    print(f"–ù–æ–≤—ã—Ö issues –¥–ª—è summary+labels: {len(need_keys_sl)}")
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as ex:
        for ikey, data in ex.map(fetch_issue_summary_labels, need_keys_sl):
            summary_labels_cache[ikey] = data
    save_json(summary_labels_cache_path, summary_labels_cache)

# –ü–æ–¥—Å—Ç—Ä–∞—Ö—É–µ–º —Å—Ç–∞—Ä—ã–π summary_cache: –µ—Å–ª–∏ —Ç–∞–º –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å –≤ summary_labels_cache ‚Äî –¥–æ–∑–∞–ø–æ–ª–Ω–∏–º
to_backfill = [k for k in all_issue_keys if k not in summary_cache and k in summary_labels_cache]
if to_backfill:
    for k in to_backfill:
        summary_cache[k] = summary_labels_cache.get(k, {}).get("summary", "") or ""
    save_json(summary_cache_path, summary_cache)

# ========= 4) –ù–æ—Ä–º—ã —Ä–∞–±–æ—á–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ =========
cal = Russia()
month_norms = {}
for m in range(1, 13):
    start = datetime(YEAR, m, 1)
    end = (datetime(YEAR, m+1, 1) - timedelta(days=1)) if m < 12 else datetime(YEAR, 12, 31)
    month_norms[f"{YEAR}-{m:02d}"] = cal.get_working_days_delta(start.date(), end.date())

# ========= 5) –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ =========
rows = []
filled_cnt = 0
for w in worklogs:
    issue = w.get("issue") or {}
    attrs = w.get("attributes") or {}
    activity_type = (attrs.get("_–¢–∏–ø–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏_", {}) or {}).get("value")

    started = pd.to_datetime(w.get("started"), errors="coerce")
    ym = started.strftime("%Y-%m") if pd.notnull(started) else None
    mmYYYY = started.strftime("%m/%Y") if pd.notnull(started) else None

    hours = (w.get("timeSpentSeconds") or 0) / 3600.0
    days  = hours / 8.0
    fte   = (days / month_norms.get(ym, 20)) if ym else None

    issue_key   = issue.get("key")
    project_key = issue.get("projectKey")
    issue_type  = issue.get("issueType")
    account_key = issue.get("accountKey")

    # Summary/Labels –∏–∑ –∫—ç—à–∞
    sl = summary_labels_cache.get(issue_key) or {"summary": "", "labels": ""}
    summary_for_report = sl.get("summary", "") or ""
    labels_for_report  = sl.get("labels", "") or ""

    if FILL_ACCOUNT_ENABLED and project_key in FILL_ACCOUNT_PROJECTS and not str(account_key or "").strip():
        # –¥–ª—è –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è account –∏—Å–ø–æ–ª—å–∑—É–µ–º summary (–∏–∑ –ª—é–±–æ–≥–æ –∫—ç—à–∞)
        smry = summary_for_report or summary_cache.get(issue_key) or issue_key
        account_key = f"{FILL_ACCOUNT_PREFIX}{sanitize_for_account(smry)}" if FILL_ACCOUNT_PREFIX else sanitize_for_account(smry)
        filled_cnt += 1

    rows.append({
        "IssueKey_tmp": issue_key,                 # –¥–ª—è —Å—Å—ã–ª–∫–∏ (–Ω–µ –ø–æ–ø–∞–¥—ë—Ç –≤ –∏—Ç–æ–≥)
        "Worker Name":  worker_map.get(w.get("worker"), w.get("worker")),
        "Account Key":  account_key,
        "Project Key":  project_key,
        "Issue Type":   issue_type,
        "Activity Type":activity_type,
        "Started (mm/yyyy)": mmYYYY,
        "Time Spent (h)": round(hours, 2),
        "FTE":                round(fte, 3) if fte is not None else None,
        "Summary":            summary_for_report,
        "Labels":             labels_for_report,
    })

print(f"–ê–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ Account Key –∏–∑ summary: {filled_cnt} –∑–∞–¥–∞—á.")

df = pd.DataFrame(rows)

# ========= 6) –†—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è + –ø–æ—Ä—è–¥–æ–∫ + —Å—Å—ã–ª–∫–∞ =========
# –°—Å—ã–ª–∫–∞
df.insert(0, "–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É", df["IssueKey_tmp"].map(lambda k: hyperlink_formula(JIRA_URL, k)))
df.drop(columns=["IssueKey_tmp"], inplace=True)

# RU –Ω–∞–∑–≤–∞–Ω–∏—è
df.rename(columns={
    "Worker Name":        "–°–æ—Ç—Ä—É–¥–Ω–∏–∫",
    "Account Key":        "–°–µ—Ä–≤–∏—Å",
    "Project Key":        "–ü—Ä–æ–µ–∫—Ç",
    "Issue Type":         "–¢–∏–ø –∑–∞–¥–∞—á–∏",
    "Activity Type":      "–¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
    "Started (mm/yyyy)":  "–ú–µ—Å—è—Ü",
    "Time Spent (h)":     "–ß–∞—Å—ã —Ñ–∞–∫—Ç",
    "FTE":                "FTE —Ñ–∞–∫—Ç",
    "Summary":            "–û–ø–∏—Å–∞–Ω–∏–µ",
    "Labels":             "–ú–µ—Ç–∫–∏",
}, inplace=True)

# –ø–æ—Ä—è–¥–æ–∫ (–±–µ–∑ ¬´–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–õ–æ–≥–∏–Ω¬ª)
RU_ORDER = ["–¢–∏–ø –∑–∞–¥–∞—á–∏","–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É","–û–ø–∏—Å–∞–Ω–∏–µ","–ú–µ—Ç–∫–∏","–ü—Ä–æ–µ–∫—Ç","–°–µ—Ä–≤–∏—Å","–ú–µ—Å—è—Ü",
            "–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
            "–¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏","–ß–∞—Å—ã —Ñ–∞–∫—Ç","FTE —Ñ–∞–∫—Ç","–°—Ç–∞–≤–∫–∞","–°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ"]
df = df[[c for c in RU_ORDER if c in df.columns] + [c for c in df.columns if c not in RU_ORDER]]

# ====== RATES ENRICHMENT (added) ======
try:
    _rates_df = load_rate_directory_or_none(RATES_FOLDER, RATES_PATTERN)
    df = enrich_with_rates(df, _rates_df)
    df = reorder_final(df)
except Exception as _e:
    print(f"[RATES] –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Å—Ç–∞–≤–∫–∞–º–∏: {_e}")
# ======================================

# ========= 7) RAW Excel =========
stamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
raw_path = WL_DIR / f"worklogs_{stamp}.xlsx"
with pd.ExcelWriter(raw_path, engine="openpyxl") as w:
    df.to_excel(w, index=False, sheet_name="Sheet1")
if EXCEL_AUTOFIT:
    excel_autofit_smart(raw_path)
print(f"‚úÖ RAW: {raw_path}")

# ========= 8) Canonical =========
columns_map = load_columns_map(COLUMNS_MAP_PATH)
df_can, canon_log = canonize_fact(df, columns_map, coerce_types=True)
# —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
if "–ú–µ—Å—è—Ü" in df_can.columns:     df_can["–ú–µ—Å—è—Ü"] = df_can["–ú–µ—Å—è—Ü"].map(to_mmYYYY)
if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫" in df_can.columns: df_can["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"] = df_can["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].map(norm_text)
if "–°–µ—Ä–≤–∏—Å" in df_can.columns:    df_can["–°–µ—Ä–≤–∏—Å"] = df_can["–°–µ—Ä–≤–∏—Å"].map(norm_text)
# –Ω–∞ –≤—Å—è–∫–∏–π ‚Äî –≤–¥—Ä—É–≥ –∫–∞–Ω–æ–Ω –¥–æ–±–∞–≤–∏—Ç –ª–∏—à–Ω–∏–π —Å—Ç–æ–ª–±–µ—Ü –ª–æ–≥–∏–Ω–∞, —É–¥–∞–ª–∏–º
if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–õ–æ–≥–∏–Ω" in df_can.columns:
    df_can.drop(columns=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–õ–æ–≥–∏–Ω"], inplace=True, errors="ignore")
# –∏ –ø–æ—Ä—è–¥–æ–∫ –∫–∞–∫ –≤ RAW
df_can = df_can[[c for c in RU_ORDER if c in df_can.columns] + [c for c in df_can.columns if c not in RU_ORDER]]

can_path = WL_OUT_DIR / f"worklogs_canonical_{stamp}.xlsx"
with pd.ExcelWriter(can_path, engine="openpyxl") as w:
    df_can.to_excel(w, index=False, sheet_name="worklogs")
    if not canon_log.empty:
        canon_log.to_excel(w, index=False, sheet_name="canonical_log")
if EXCEL_AUTOFIT:
    excel_autofit_smart(can_path)

write_schema_json(can_path.with_suffix(".schema.json"), {"worklogs": df_can})
print(f"‚úÖ Canonical: {can_path}")
print(f"üßæ Schema:    {can_path.with_suffix('.schema.json')}")
