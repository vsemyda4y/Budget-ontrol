# -*- coding: utf-8 -*-
r"""
Worklog.py ‚Äî Tempo ‚Üí —Ä—É—Å—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏, –±–µ–∑ ¬´–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–õ–æ–≥–∏–Ω¬ª.
+ Summary (–û–ø–∏—Å–∞–Ω–∏–µ) –∏ Labels (–ú–µ—Ç–∫–∏) –∏–∑ Jira Issue fields.
+ FTE –ø–æ—Å—Ç—Ä–æ—á–Ω–æ (–∫–∞–∫ –±—ã–ª–æ) –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø–æ–º–µ—Å—è—á–Ω—ã–π FTE –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ª–∏—Å—Ç–µ (monthly_fte).

–ü–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫:
  –¢–∏–ø –∑–∞–¥–∞—á–∏, –°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É, –û–ø–∏—Å–∞–Ω–∏–µ, –ú–µ—Ç–∫–∏, –ü—Ä–æ–µ–∫—Ç, –°–µ—Ä–≤–∏—Å, –ú–µ—Å—è—Ü,
  –°–æ—Ç—Ä—É–¥–Ω–∏–∫, –°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è, –°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö, –°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ,
  –¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, –ß–∞—Å—ã —Ñ–∞–∫—Ç, FTE —Ñ–∞–∫—Ç, –°—Ç–∞–≤–∫–∞, –°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ
"""

import os, re, json, time, requests, pandas as pd
from workalendar.europe import Russia
from datetime import datetime, timedelta
import concurrent.futures
from pathlib import Path
from typing import Dict, Tuple, List, Optional

from canon_utils import load_columns_map, canonize_fact, write_schema_json, norm_text, to_mmYYYY

# ========= ENV =========
JIRA_URL       = os.getenv("JIRA_URL", "").strip()
BEARER_TOKEN   = os.getenv("JIRA_TEMPO_TOKEN", "").strip()
YEAR           = int(os.getenv("YEAR", datetime.now().year))
TIMEZONE       = os.getenv("TIMEZONE", "Europe/Moscow").strip()

# –ù–æ—Ä–º—ã: 0 -> —Å—á–∏—Ç–∞–µ–º –ø–æ workalendar;
# –µ—Å–ª–∏ >0, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞—Å—ã/–º–µ—Å—è—Ü (–Ω–∞–ø—Ä–∏–º–µ—Ä, 160)
MONTH_NORM_HOURS = float(os.getenv("MONTH_NORM_HOURS", "0"))

# –¢–æ—á–µ—á–Ω—ã–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–æ—Ä–º—ã —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π: "YYYY-MM=18;YYYY-MM=21"
MONTH_NORM_OVERRIDE = os.getenv("MONTH_NORM_OVERRIDE", "")

if not JIRA_URL or not BEARER_TOKEN:
    raise SystemExit("ENV JIRA_URL –∏–ª–∏ JIRA_TEMPO_TOKEN –Ω–µ –∑–∞–¥–∞–Ω—ã")

FILL_ACCOUNT_ENABLED  = os.getenv("FILL_ACCOUNT_ENABLED", "1") == "1"
FILL_ACCOUNT_PROJECTS = [p.strip() for p in os.getenv("FILL_ACCOUNT_PROJECTS", "INT").split(";") if p.strip()]
FILL_ACCOUNT_PREFIX   = os.getenv("FILL_ACCOUNT_PREFIX", "INT: ")
EXCEL_AUTOFIT         = os.getenv("EXCEL_AUTOFIT", "1") == "1"  # —É–º–Ω—ã–π –∞–≤—Ç–æ—Ñ–∏—Ç

WL_DIR     = Path(r"C:\Users\zaytsev_ra2\PycharmProjects\TCO\WL")
WL_OUT_DIR = WL_DIR / "–†–µ–∑—É–ª—å—Ç–∞—Ç"

# ====== Rates directory config ======
RATES_FOLDER  = Path(r"C:\Users\zaytsev_ra2\PycharmProjects\–ö–æ–Ω—Ç—Ä–æ–ª—å –±—é–¥–∂–µ—Ç–∞\–°—Ç–∞–≤–∫–∏")
RATES_PATTERN = "employee_rate_directory_*.xlsx"  # mask for the rates files

# ================= helpers for rates =================
def _pick_latest_file(folder: Path, pattern: str) -> Optional[Path]:
    files = sorted(Path(folder).glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)
    return files[0] if files else None

def _pick_col(df: pd.DataFrame, candidates):
    low = {c.lower(): c for c in df.columns}
    for cand in candidates:
        cl = cand.strip().lower()
        if cl in low:
            return low[cl]
    for cand in candidates:
        cl = cand.strip().lower()
        for c in df.columns:
            if cl in c.strip().lower():
                return c
    return None

def normalize_employee_name(name: str) -> str:
    if name is None:
        return ""
    s = str(name)
    s = s.replace("\u00a0", " ").replace("\u202f", " ")
    s = re.sub(r'\s*(\[[^\]]+\]\s*)+$', '', s).strip()
    s = re.sub(r'\s+', ' ', s)
    return s

def load_rate_directory_or_none(folder: Path, pattern: str) -> Optional[pd.DataFrame]:
    path = _pick_latest_file(folder, pattern)
    if path is None:
        print(f"[RATES] –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ '{folder}' –ø–æ –º–∞—Å–∫–µ '{pattern}' ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
        return None
    try:
        rdf = pd.read_excel(path)
    except Exception as e:
        print(f"[RATES] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è '{path}': {e} ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
        return None
    emp  = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–§–ò–û","–°–æ—Ç—Ä—É–¥–Ω–∏–∫ –§–ò–û"])
    rate = _pick_col(rdf, ["–°—Ç–∞–≤–∫–∞","–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á","–°—Ç–∞–≤–∫–∞ (‚ÇΩ/—á)","–°—Ç–æ–∏–º–æ—Å—Ç—å —á–∞—Å–∞","–¶–µ–Ω–∞ —á–∞—Å–∞"])
    cc   = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö","–¶–ö","Cost Center","–°–¶–ö"])
    dept = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ","–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ","–û—Ç–¥–µ–ª","–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç"])
    cat  = _pick_col(rdf, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è","–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞","–ö–∞—Ç–µ–≥–æ—Ä–∏—è"])
    if not emp or not rate:
        print(f"[RATES] –í '{path.name}' –Ω–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–°–æ—Ç—Ä—É–¥–Ω–∏–∫/–°—Ç–∞–≤–∫–∞) ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
        return None
    rename = {emp:"–°–æ—Ç—Ä—É–¥–Ω–∏–∫", rate:"–°—Ç–∞–≤–∫–∞"}
    if cc:   rename[cc]   = "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö"
    if dept: rename[dept] = "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"
    if cat:  rename[cat]  = "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è"
    rdf = rdf.rename(columns=rename)
    rdf["–°—Ç–∞–≤–∫–∞"] = pd.to_numeric(rdf["–°—Ç–∞–≤–∫–∞"], errors="coerce")
    rdf["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] = rdf["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].apply(normalize_employee_name)
    keep = [c for c in ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–°—Ç–∞–≤–∫–∞","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è","–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] if c in rdf.columns]
    rdf = rdf[keep].dropna(subset=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"]).drop_duplicates(subset=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"])
    print(f"[RATES] –ó–∞–≥—Ä—É–∂–µ–Ω —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ '{path.name}': {len(rdf)} —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤")
    return rdf

def enrich_with_rates(df: pd.DataFrame, rates: Optional[pd.DataFrame]) -> pd.DataFrame:
    if rates is None or rates.empty:
        return df
    if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫" not in df.columns:
        emp = _pick_col(df, ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–§–ò–û","–°–æ—Ç—Ä—É–¥–Ω–∏–∫ –§–ò–û"])
        if not emp:
            print("[RATES] –ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ '–°–æ—Ç—Ä—É–¥–Ω–∏–∫' ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞–≤–∫–∏.")
            return df
        if emp != "–°–æ—Ç—Ä—É–¥–Ω–∏–∫":
            df = df.rename(columns={emp:"–°–æ—Ç—Ä—É–¥–Ω–∏–∫"})
    df["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] = df["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].apply(normalize_employee_name)
    if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm" not in rates.columns:
        rates["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"] = rates["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].apply(normalize_employee_name)
    merged = df.merge(rates, on="–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm", how="left", suffixes=("", "_rate"))
    if "–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" in merged.columns and "–°—Ç–∞–≤–∫–∞" not in merged.columns:
        merged = merged.rename(columns={"–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á": "–°—Ç–∞–≤–∫–∞"})
    hours_col = _pick_col(merged, ["–ß–∞—Å—ã —Ñ–∞–∫—Ç","–ß–∞—Å—ã","Hours","–í—Ä–µ–º—è, —á"])
    if hours_col:
        rate_col = "–°—Ç–∞–≤–∫–∞" if "–°—Ç–∞–≤–∫–∞" in merged.columns else ("–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" if "–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" in merged.columns else None)
        if rate_col:
            merged["–°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ"] = (merged[hours_col].fillna(0) * merged[rate_col].fillna(0)).round(2)
    if "–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á" in merged.columns and "–°—Ç–∞–≤–∫–∞" in merged.columns:
        merged = merged.drop(columns=["–°—Ç–∞–≤–∫–∞, ‚ÇΩ/—á"])
    for c in ["–°–æ—Ç—Ä—É–¥–Ω–∏–∫_rate","–°–æ—Ç—Ä—É–¥–Ω–∏–∫_norm"]:
        if c in merged.columns:
            merged = merged.drop(columns=[c])
    return merged

# ====== Final column order helper ======
FINAL_ORDER = ["–¢–∏–ø –∑–∞–¥–∞—á–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É", "–û–ø–∏—Å–∞–Ω–∏–µ", "–ú–µ—Ç–∫–∏", "–ü—Ä–æ–µ–∫—Ç", "–°–µ—Ä–≤–∏—Å", "–ú–µ—Å—è—Ü",
               "–°–æ—Ç—Ä—É–¥–Ω–∏–∫", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
               "–¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "–ß–∞—Å—ã —Ñ–∞–∫—Ç", "FTE —Ñ–∞–∫—Ç", "–°—Ç–∞–≤–∫–∞", "–°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ"]

def reorder_final(df: pd.DataFrame) -> pd.DataFrame:
    keep = [c for c in FINAL_ORDER if c in df.columns]
    rest = [c for c in df.columns if c not in keep]
    return df[keep + rest]

# ============================================
WL_OUT_DIR.mkdir(parents=True, exist_ok=True)
COLUMNS_MAP_PATH = Path(__file__).with_name("columns_map.json")

HEADERS = {
    "Authorization": f"Bearer {BEARER_TOKEN}",
    "Content-Type": "application/json",
    "Accept": "application/json"
}

# ========= –£–º–Ω—ã–π –∞–≤—Ç–æ—Ñ–∏—Ç =========
WIDTH_OVERRIDES = {"–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É": 16, "–°–µ—Ä–≤–∏—Å": 34, "–û–ø–∏—Å–∞–Ω–∏–µ": 60}
CLAMP_MIN, CLAMP_MAX = 6, 60

def _visible_text(cell_val):
    if not isinstance(cell_val, str):
        return "" if cell_val is None else str(cell_val)
    s = cell_val.strip()
    if not s.startswith("="):
        return s
    m = re.match(r'^\s*=\s*HYPERLINK\s*\(\s*"[^"]*"\s*[,;]\s*"([^"]*)"\s*\)\s*$', s, re.IGNORECASE)
    return m.group(1) if m else s

def excel_autofit_smart(path: Path):
    try:
        from openpyxl import load_workbook
        wb = load_workbook(path)
        for ws in wb.worksheets:
            headers = {cell.column: (cell.value or "") for cell in ws[1]}
            for col in ws.columns:
                col_idx = col[0].column
                head = str(headers.get(col_idx, "")).strip()
                if head in WIDTH_OVERRIDES:
                    ws.column_dimensions[col[0].column_letter].width = WIDTH_OVERRIDES[head]
                    continue
                max_len = max(len(_visible_text(c.value)) for c in col)
                max_len = max(max_len, len(head))
                ws.column_dimensions[col[0].column_letter].width = max(CLAMP_MIN, min(max_len + 2, CLAMP_MAX))
        wb.save(path)
    except Exception as e:
        print(f"[autofit] –ø—Ä–æ–ø—É—â–µ–Ω–æ: {e}")

# ========= misc helpers =========
def load_json(path: Path) -> Dict[str, str]:
    if path.exists():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_json(path: Path, data: Dict[str, str]):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
    except Exception as e:
        print(f"[cache] –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {path.name}: {e}")

def get_display_name(wid: str) -> Tuple[str, str]:
    url = f"{JIRA_URL}/rest/api/2/user?key={wid}"
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        if r.status_code == 200:
            user = r.json()
            return wid, user.get("displayName", wid)
    except Exception:
        time.sleep(0.2)
    return wid, wid

def fetch_issue_summary_labels(issue_key: str) -> Tuple[str, Dict[str, str]]:
    url = f"{JIRA_URL}/rest/api/2/issue/{issue_key}?fields=summary,labels"
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        if r.status_code == 200:
            data = r.json() or {}
            fields = data.get("fields") or {}
            return issue_key, {
                "summary": fields.get("summary", "") or "",
                "labels": ",".join(fields.get("labels") or [])
            }
    except Exception:
        time.sleep(0.2)
    return issue_key, {"summary": "", "labels": ""}

def sanitize_for_account(title: str) -> str:
    if title is None:
        return ""
    s = " ".join(str(title).replace("\n", " ").replace("\r", " ").split())
    return s[:100]

def hyperlink_formula(jira_base_url: str, key: str) -> str:
    if not key:
        return ""
    k = str(key).strip().replace('"', '""')
    return f'=HYPERLINK("{jira_base_url.rstrip('/')}/browse/{k}","{k}")'

# ========= –ù–æ—Ä–º—ã —Ä–∞–±–æ—á–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ =========
def build_month_norms(year: int) -> Dict[str, int]:
    cal = Russia()
    norms = {}
    for m in range(1, 13):
        start = datetime(year, m, 1)
        end = (datetime(year, m+1, 1) - timedelta(days=1)) if m < 12 else datetime(year, 12, 31)
        norms[f"{year}-{m:02d}"] = int(cal.get_working_days_delta(start.date(), end.date()))
    # –¢–æ—á–µ—á–Ω—ã–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ ENV
    if MONTH_NORM_OVERRIDE:
        for part in MONTH_NORM_OVERRIDE.split(";"):
            part = part.strip()
            if not part or "=" not in part:
                continue
            ym, val = part.split("=", 1)
            ym, val = ym.strip(), val.strip()
            try:
                norms[ym] = int(val)
            except:
                pass
    return norms

month_norms = build_month_norms(YEAR)

def month_norm_days(ym: str) -> int:
    # ym = "YYYY-MM"
    return int(month_norms.get(ym, 20))

# ========= 1) Tempo =========
tempo_url = f"{JIRA_URL}/rest/tempo-timesheets/4/worklogs/search"
payload = {"from": f"{YEAR}-01-01", "to": f"{YEAR}-12-31"}
resp = requests.post(tempo_url, json=payload, headers=HEADERS, timeout=120)
if resp.status_code != 200:
    print("–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Tempo:", resp.status_code, resp.text); raise SystemExit(1)
worklogs = resp.json() or []

# ========= 2) –ö–µ—à –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π =========
worker_ids = list({w.get("worker") for w in worklogs if w.get("worker")})
worker_map_path = WL_DIR / 'worker_map.json'
worker_map = load_json(worker_map_path)
new_ids = [wid for wid in worker_ids if wid and wid not in worker_map]
if new_ids:
    print(f"–ù–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(new_ids)} ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as ex:
        for wid, name in ex.map(get_display_name, new_ids):
            worker_map[wid] = name
    save_json(worker_map_path, worker_map)

# ========= 3) –ö–µ—à summary+labels =========
summary_labels_cache_path = WL_DIR / "issue_summary_labels_cache.json"
summary_labels_cache = load_json(summary_labels_cache_path)

all_issue_keys = sorted({(w.get("issue") or {}).get("key") for w in worklogs if (w.get("issue") or {}).get("key")})
need_keys_sl = [k for k in all_issue_keys if k and k not in summary_labels_cache]
if need_keys_sl:
    print(f"–ù–æ–≤—ã—Ö issues –¥–ª—è summary+labels: {len(need_keys_sl)}")
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as ex:
        for ikey, data in ex.map(fetch_issue_summary_labels, need_keys_sl):
            summary_labels_cache[ikey] = data
    save_json(summary_labels_cache_path, summary_labels_cache)

# ========= 4) –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ =========
rows = []
filled_cnt = 0

# –§—É–Ω–∫—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã —Å TZ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π:
def parse_started_ts(value) -> Optional[datetime]:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç Tempo started ‚Üí –ª–æ–∫–∞–ª—å–Ω–æ–µ –Ω–∞–∏–≤–Ω–æ–µ –≤—Ä–µ–º—è –≤ –∑–∞–¥–∞–Ω–Ω–æ–º TIMEZONE.
    –£—Å—Ç—Ä–∞–Ω—è–µ—Ç —É—Ç–µ—á–∫–∏ –≤ —Å–æ—Å–µ–¥–Ω–∏–µ –º–µ—Å—è—Ü—ã –∏–∑-–∑–∞ TZ.
    """
    try:
        ts = pd.to_datetime(value, utc=True, errors="coerce")
        if pd.isna(ts):
            return None
        ts = ts.tz_convert(TIMEZONE).tz_localize(None)
        return ts.to_pydatetime()
    except Exception:
        return None

for w in worklogs:
    issue = w.get("issue") or {}
    attrs = w.get("attributes") or {}
    activity_type = (attrs.get("_–¢–∏–ø–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏_", {}) or {}).get("value")

    started_dt = parse_started_ts(w.get("started"))
    ym = started_dt.strftime("%Y-%m") if started_dt else None
    mmYYYY = started_dt.strftime("%m/%Y") if started_dt else None

    hours = (w.get("timeSpentSeconds") or 0) / 3600.0
    # –ü–æ—Å—Ç—Ä–æ—á–Ω—ã–π FTE:
    if MONTH_NORM_HOURS > 0:
        fte = hours / MONTH_NORM_HOURS
    else:
        days_norm = month_norm_days(ym) if ym else 20
        fte = (hours / 8.0) / days_norm if days_norm else None

    issue_key   = (issue.get("key") or "").strip()
    project_key = issue.get("projectKey")
    issue_type  = issue.get("issueType")
    account_key = issue.get("accountKey")

    # Summary/Labels –∏–∑ –∫—ç—à–∞
    sl = summary_labels_cache.get(issue_key) or {"summary": "", "labels": ""}
    summary_for_report = sl.get("summary", "") or ""
    labels_for_report  = sl.get("labels", "") or ""

    if FILL_ACCOUNT_ENABLED and project_key in FILL_ACCOUNT_PROJECTS and not str(account_key or "").strip():
        smry = summary_for_report or issue_key
        account_key = f"{FILL_ACCOUNT_PREFIX}{sanitize_for_account(smry)}" if FILL_ACCOUNT_PREFIX else sanitize_for_account(smry)
        filled_cnt += 1

    rows.append({
        "IssueKey_tmp": issue_key,                 # –¥–ª—è —Å—Å—ã–ª–∫–∏ (–Ω–µ –ø–æ–ø–∞–¥—ë—Ç –≤ –∏—Ç–æ–≥)
        "Worker Name":  worker_map.get(w.get("worker"), w.get("worker")),
        "Account Key":  account_key,
        "Project Key":  project_key,
        "Issue Type":   issue_type,
        "Activity Type":activity_type,
        "Started (mm/yyyy)": mmYYYY,
        "Time Spent (h)": round(hours, 2),
        "FTE":                round(fte, 3) if fte is not None else None,
        "Summary":            summary_for_report,
        "Labels":             labels_for_report,
        "_ym":                ym,                 # –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    })

print(f"–ê–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ Account Key –∏–∑ summary: {filled_cnt} –∑–∞–¥–∞—á.")

df = pd.DataFrame(rows)

# ========= 5) –†—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è + –ø–æ—Ä—è–¥–æ–∫ + —Å—Å—ã–ª–∫–∞ =========
df.insert(0, "–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É", df["IssueKey_tmp"].map(lambda k: hyperlink_formula(JIRA_URL, k)))
df.drop(columns=["IssueKey_tmp"], inplace=True)

df.rename(columns={
    "Worker Name":        "–°–æ—Ç—Ä—É–¥–Ω–∏–∫",
    "Account Key":        "–°–µ—Ä–≤–∏—Å",
    "Project Key":        "–ü—Ä–æ–µ–∫—Ç",
    "Issue Type":         "–¢–∏–ø –∑–∞–¥–∞—á–∏",
    "Activity Type":      "–¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
    "Started (mm/yyyy)":  "–ú–µ—Å—è—Ü",
    "Time Spent (h)":     "–ß–∞—Å—ã —Ñ–∞–∫—Ç",
    "FTE":                "FTE —Ñ–∞–∫—Ç",
    "Summary":            "–û–ø–∏—Å–∞–Ω–∏–µ",
    "Labels":             "–ú–µ—Ç–∫–∏",
}, inplace=True)

RU_ORDER = ["–¢–∏–ø –∑–∞–¥–∞—á–∏","–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É","–û–ø–∏—Å–∞–Ω–∏–µ","–ú–µ—Ç–∫–∏","–ü—Ä–æ–µ–∫—Ç","–°–µ—Ä–≤–∏—Å","–ú–µ—Å—è—Ü",
            "–°–æ—Ç—Ä—É–¥–Ω–∏–∫","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ö–∞—Ç–µ–≥–æ—Ä–∏—è","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–¶–ö","–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
            "–¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏","–ß–∞—Å—ã —Ñ–∞–∫—Ç","FTE —Ñ–∞–∫—Ç","–°—Ç–∞–≤–∫–∞","–°—Ç–æ–∏–º–æ—Å—Ç—å, ‚ÇΩ"]

df = df[[c for c in RU_ORDER if c in df.columns] + [c for c in df.columns if c not in RU_ORDER]]

# ====== RATES ENRICHMENT ======
try:
    _rates_df = load_rate_directory_or_none(RATES_FOLDER, RATES_PATTERN)
    df = enrich_with_rates(df, _rates_df)
    df = reorder_final(df)
except Exception as _e:
    print(f"[RATES] –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Å—Ç–∞–≤–∫–∞–º–∏: {_e}")

# ========= 6) –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è FTE –ø–æ –º–µ—Å—è—Ü—É/—Å–æ—Ç—Ä—É–¥–Ω–∏–∫—É =========
# –ü—Ä–∏–≤–æ–¥–∏–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–ª—é—á –º–µ—Å—è—Ü–∞
if "_ym" not in df.columns:
    df["_ym"] = pd.to_datetime(df["–ú–µ—Å—è—Ü"], format="%m/%Y", errors="coerce").dt.strftime("%Y-%m")

# 1) –°—É–º–º–∏—Ä—É–µ–º —á–∞—Å—ã
monthly = (
    df.groupby(["_ym", "–°–æ—Ç—Ä—É–¥–Ω–∏–∫"], dropna=False)["–ß–∞—Å—ã —Ñ–∞–∫—Ç"]
      .sum()
      .reset_index()
      .rename(columns={"_ym": "–ú–µ—Å—è—Ü (YYYY-MM)"})
)

# 2) –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ—Ä–º—É –∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º FTE –º–µ—Å—è—Ü
if MONTH_NORM_HOURS > 0:
    monthly["–ù–æ—Ä–º–∞_–¥–Ω–µ–π"]  = None
    monthly["–ù–æ—Ä–º–∞_—á–∞—Å–æ–≤"] = MONTH_NORM_HOURS
    monthly["FTE –º–µ—Å—è—Ü"]   = (monthly["–ß–∞—Å—ã —Ñ–∞–∫—Ç"] / MONTH_NORM_HOURS).round(3)
else:
    monthly["–ù–æ—Ä–º–∞_–¥–Ω–µ–π"]  = monthly["–ú–µ—Å—è—Ü (YYYY-MM)"].map(month_norm_days).astype("Int64")
    monthly["–ù–æ—Ä–º–∞_—á–∞—Å–æ–≤"] = (8 * monthly["–ù–æ—Ä–º–∞_–¥–Ω–µ–π"]).astype("Int64")
    monthly["FTE –º–µ—Å—è—Ü"]   = (monthly["–ß–∞—Å—ã —Ñ–∞–∫—Ç"] / monthly["–ù–æ—Ä–º–∞_—á–∞—Å–æ–≤"]).round(3)

# ========= 7) RAW Excel =========
stamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
raw_path = WL_DIR / f"worklogs_{stamp}.xlsx"
with pd.ExcelWriter(raw_path, engine="openpyxl") as w:
    df.to_excel(w, index=False, sheet_name="worklogs_raw")
    monthly.to_excel(w, index=False, sheet_name="monthly_fte")
if EXCEL_AUTOFIT:
    excel_autofit_smart(raw_path)
print(f"‚úÖ RAW: {raw_path}")

# ========= 8) Canonical =========
columns_map = load_columns_map(COLUMNS_MAP_PATH)
df_can, canon_log = canonize_fact(df, columns_map, coerce_types=True)
# —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
if "–ú–µ—Å—è—Ü" in df_can.columns:     df_can["–ú–µ—Å—è—Ü"] = df_can["–ú–µ—Å—è—Ü"].map(to_mmYYYY)
if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫" in df_can.columns: df_can["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"] = df_can["–°–æ—Ç—Ä—É–¥–Ω–∏–∫"].map(norm_text)
if "–°–µ—Ä–≤–∏—Å" in df_can.columns:    df_can["–°–µ—Ä–≤–∏—Å"] = df_can["–°–µ—Ä–≤–∏—Å"].map(norm_text)
if "–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–õ–æ–≥–∏–Ω" in df_can.columns:
    df_can.drop(columns=["–°–æ—Ç—Ä—É–¥–Ω–∏–∫:–õ–æ–≥–∏–Ω"], inplace=True, errors="ignore")
df_can = df_can[[c for c in RU_ORDER if c in df_can.columns] + [c for c in df_can.columns if c not in RU_ORDER]]

can_path = WL_OUT_DIR / f"worklogs_canonical_{stamp}.xlsx"
with pd.ExcelWriter(can_path, engine="openpyxl") as w:
    df_can.to_excel(w, index=False, sheet_name="worklogs")
    monthly.to_excel(w, index=False, sheet_name="monthly_fte")  # –∫–ª–∞–¥—ë–º –∏ —Å—é–¥–∞
    if not canon_log.empty:
        canon_log.to_excel(w, index=False, sheet_name="canonical_log")
if EXCEL_AUTOFIT:
    excel_autofit_smart(can_path)

write_schema_json(can_path.with_suffix(".schema.json"), {"worklogs": df_can, "monthly_fte": monthly})
print(f"‚úÖ Canonical: {can_path}")
print(f"üßæ Schema:    {can_path.with_suffix('.schema.json')}")

# ========= 9) –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (–ø–æ –∂–µ–ª–∞–Ω–∏—é) =========
try:
    ym_check = f"{YEAR}-05"
    may_hours = monthly.loc[monthly["–ú–µ—Å—è—Ü (YYYY-MM)"].eq(ym_check), "–ß–∞—Å—ã —Ñ–∞–∫—Ç"].sum()
    if MONTH_NORM_HOURS > 0:
        denom = MONTH_NORM_HOURS
    else:
        denom = 8 * month_norm_days(ym_check)
    if denom:
        print(f"[CHECK] –ú–∞–π {YEAR}: Œ£—á–∞—Å—ã={may_hours}, –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å={denom}, FTE={may_hours/denom:.3f}")
except Exception:
    pass
